# 关键点划分依据说明

## 概述

本文档说明 MediaPipe 如何检测和划分面部、手部和姿态关键点，以及划分的依据。

## 划分依据

### 核心原理

MediaPipe 使用**三个独立的深度学习模型**来检测不同的人体部位：

1. **Face Mesh 模型** - 检测面部关键点
2. **Hands 模型** - 检测手部关键点
3. **Pose 模型** - 检测身体姿态关键点

这些模型是**独立的**，各自专门针对特定的人体部位进行优化。

## 各部位检测说明

### 1. 面部（Face）- 68 个关键点

**检测模型**: `mp.solutions.face_mesh.FaceMesh`

**工作原理**:
- MediaPipe Face Mesh 模型检测人脸的 **468 个3D 关键点**
- 我们从中选择 **68 个标准面部关键点**（符合常见的面部关键点标准）
- 映射到标准的 68 点面部关键点格式

**检测依据**:
- 输入: 完整的 RGB 图像
- 模型: 专门训练的面部检测模型
- 输出: 面部区域内的关键点（468 点 → 68 点映射）

**68 点分布**:
```
- 面部轮廓: 17 个点 (下巴到额头)
- 右眉毛: 5 个点
- 左眉毛: 5 个点
- 鼻子: 9 个点
- 右眼: 6 个点
- 左眼: 6 个点
- 嘴巴外轮廓: 12 个点
- 嘴巴内轮廓: 8 个点
总计: 68 个点
```

**关键代码**:
```python
# 初始化 Face Mesh 模型
self.face_mesh = mp.solutions.face_mesh.FaceMesh(
    static_image_mode=True,      # 静态图像模式
    max_num_faces=1,             # 最多检测 1 张脸
    refine_landmarks=True,       # 精细化关键点
    min_detection_confidence=0.5
)

# 处理图像
face_results = self.face_mesh.process(image_rgb)

# 提取 68 个关键点（从 468 点映射）
if face_results.multi_face_landmarks:
    face_kp = self.extract_face_keypoints(
        image_rgb,
        face_results.multi_face_landmarks[0]
    )
```

### 2. 手部（Hands）- 每只手 21 个关键点

**检测模型**: `mp.solutions.hands.Hands`

**工作原理**:
- MediaPipe Hands 模型检测手部的 **21 个3D 关键点**（每只手）
- 可以同时检测**最多 2 只手**（左手和右手）
- 自动区分左右手（基于图像中的位置）

**检测依据**:
- 输入: 完整的 RGB 图像
- 模型: 专门训练的手部检测模型
- 输出: 手部区域内的关键点

**21 点分布**（每只手）:
```
- 手腕: 1 个点 (点 0)
- 拇指: 4 个点 (点 1-4)
- 食指: 4 个点 (点 5-8)
- 中指: 4 个点 (点 9-12)
- 无名指: 4 个点 (点 13-16)
- 小指: 4 个点 (点 17-20)
总计: 21 个点
```

**关键代码**:
```python
# 初始化 Hands 模型
self.hands = mp.solutions.hands.Hands(
    static_image_mode=True,        # 静态图像模式
    max_num_hands=2,               # 最多检测 2 只手
    min_detection_confidence=0.5,
    min_tracking_confidence=0.5
)

# 处理图像
hand_results = self.hands.process(image_rgb)

# 提取手部关键点
if hand_results.multi_hand_landmarks:
    left_hand_kp, right_hand_kp = self.extract_hand_keypoints(
        image_rgb,
        hand_results.multi_hand_landmarks
    )
```

**左右手区分**:
- MediaPipe 基于图像中的位置自动区分左右手
- **注意**: MediaPipe 返回的 'Left' 和 'Right' 是从**摄像头视角**定义的
  - 图像中左侧的手 = 'Right'（从被拍摄者视角是右手）
  - 图像中右侧的手 = 'Left'（从被拍摄者视角是左手）
- 我们的代码使用检测顺序：第一个检测到的手为右手，第二个为左手

### 3. 姿态（Pose）- 33 个关键点

**检测模型**: `mp.solutions.pose.Pose`

**工作原理**:
- MediaPipe Pose 模型检测身体姿态的 **33 个3D 关键点**
- 覆盖全身主要关节和部位
- 不包括面部和手部（这些由专门的模型处理）

**检测依据**:
- 输入: 完整的 RGB 图像
- 模型: 专门训练的身体姿态检测模型
- 输出: 身体姿态关键点（不包含面部和手部细节）

**33 点分布**:
```
- 面部/头部: 4 个点 (鼻尖、左右眼、左右耳)
- 上半身: 10 个点 (肩膀、肘部、手腕、胸部)
- 躯干: 4 个点 (臀部、腰部)
- 下半身: 8 个点 (膝盖、脚踝、脚趾)
- 其他: 7 个点 (辅助点)
总计: 33 个点
```

**关键代码**:
```python
# 初始化 Pose 模型
self.pose = mp.solutions.pose.Pose(
    static_image_mode=True,         # 静态图像模式
    model_complexity=2,             # 模型复杂度（0-2，2 最准确）
    enable_segmentation=False,      # 不使用分割
    min_detection_confidence=0.5,
    min_tracking_confidence=0.5
)

# 处理图像
pose_results = self.pose.process(image_rgb)

# 提取姿态关键点
if pose_results.pose_landmarks:
    pose_kp = self.extract_pose_keypoints(
        image_rgb,
        pose_results.pose_landmarks
    )
```

## 检测流程

### 完整检测流程

```python
def extract_keypoints(self, image_path):
    """
    从图像中提取所有关键点的完整流程
    """
    # 1. 加载图像
    image = cv2.imread(image_path)
    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

    # 2. 并行检测三个部位（使用三个独立的模型）

    # 2.1 检测面部
    face_results = self.face_mesh.process(image_rgb)
    # → 如果检测到面部，提取 68 个关键点

    # 2.2 检测手部
    hand_results = self.hands.process(image_rgb)
    # → 如果检测到手部，提取最多 2 只手 × 21 个关键点

    # 2.3 检测姿态
    pose_results = self.pose.process(image_rgb)
    # → 如果检测到身体，提取 33 个关键点

    # 3. 组合结果
    return {
        'face': face_kp,        # (68, 3) or None
        'left_hand': left_kp,   # (21, 3) or None
        'right_hand': right_kp, # (21, 3) or None
        'pose': pose_kp,        # (33, 3) or None
        'image_shape': (h, w)
    }
```

## 划分依据总结

### 1. 模型独立性

三个模型是**完全独立**的：
- 各自使用不同的深度学习模型
- 各自针对特定的人体部位优化
- 各自有独立的检测流程
- 检测结果互不影响

### 2. 检测区域

虽然输入都是完整图像，但每个模型专注于不同的区域：

| 模型 | 检测区域 | 关键点数量 | 特点 |
|------|---------|-----------|------|
| **Face Mesh** | 面部区域 | 68 点 | 高精度面部特征 |
| **Hands** | 手部区域 | 21 点/手 | 精确手指关节 |
| **Pose** | 全身姿态 | 33 点 | 主要关节和部位 |

### 3. 划分逻辑

划分是基于**模型的功能定位**，而非后处理：

1. **面部划分**:
   - Face Mesh 模型专门检测面部
   - 输出面部的 68 个关键点
   - 不包含手部和身体其他部位

2. **手部划分**:
   - Hands 模型专门检测手部
   - 输出每只手的 21 个关键点
   - 与面部和姿态独立检测

3. **姿态划分**:
   - Pose 模型检测全身姿态
   - 输出 33 个身体关键点
   - 不包括面部和手部的细节

### 4. 为何分开检测？

**优势**:
1. **精度更高**: 每个模型针对特定部位优化
2. **速度更快**: 可以并行处理，提高效率
3. **灵活性更好**: 可以独立使用某个部位的检测
4. **鲁棒性更强**: 某个部位未检测到不影响其他部位

## 坐标系统

### 坐标格式

所有关键点使用**归一化坐标**:
- **X, Y**: 归一化到 [0, 1]，相对于图像宽度和高度
- **Z**: 深度信息（相对于图像中心的距离，单位：像素）

例如：
- (0.5, 0.5, 0) = 图像中心
- (0.0, 0.0, 0) = 左上角
- (1.0, 1.0, 0) = 右下角

### 坐标转换

在可视化时需要转换回像素坐标：
```python
# 归一化坐标 → 像素坐标
pixel_x = normalized_x * image_width
pixel_y = normalized_y * image_height
```

## 检测失败处理

### 检测失败的情况

某个部位可能检测失败（返回 `None`）的原因：

1. **面部未检测到**:
   - 人脸被遮挡
   - 人脸太小
   - 人脸不在画面中
   - 光照条件差

2. **手部未检测到**:
   - 手被遮挡
   - 手在画面外
   - 手太小
   - 手部动作过快

3. **姿态未检测到**:
   - 身体被遮挡
   - 身体不在画面中
   - 人物太小
   - 姿态过于复杂

### 处理策略

代码中的处理：
```python
# 如果某个部位未检测到，返回 None
if face_kp is None and left_hand_kp is None and right_hand_kp is None and pose_kp is None:
    return None  # 完全未检测到

# 如果至少检测到一个部位，返回部分结果
return {
    'face': face_kp,        # 可能是 None
    'left_hand': left_hand_kp,  # 可能是 None
    'right_hand': right_hand_kp,  # 可能是 None
    'pose': pose_kp         # 可能是 None
}
```

## 关键点数量说明

### 为什么是 143 个关键点？

```
68 (面部) + 21 (左手) + 21 (右手) + 33 (姿态) = 143 个关键点
```

**注意**: 这是**理论最大值**，实际检测到的关键点数量可能少于 143，因为：
- 某些部位可能未检测到（返回 `None`）
- 某些关键点可能在特定姿态下不可见

### 实际使用

在模型训练中，通常需要处理缺失的关键点：
- **选项 1**: 使用零填充（`np.zeros((68, 3))`）
- **选项 2**: 只使用检测到的关键点
- **选项 3**: 使用掩码标记缺失的关键点

## 可视化示例

查看实际的关键点分布：

```bash
# 可视化关键点，查看各部位的分布
python visualize_phoenix_keypoints.py phoenix_keypoints_test.pkl \
    --num_samples 5 \
    --splits train

# 生成的文件会显示：
# - sample_*_face.png - 只有面部关键点
# - sample_*_hands.png - 只有手部关键点
# - sample_*_pose.png - 只有姿态关键点
# - sample_*_full_skeleton.png - 所有关键点
```

## 相关文档

- [MediaPipe 官方文档](https://google.github.io/mediapipe/)
- [Face Mesh 模型说明](https://google.github.io/mediapipe/solutions/face_mesh)
- [Hands 模型说明](https://google.github.io/mediapipe/solutions/hands)
- [Pose 模型说明](https://google.github.io/mediapipe/solutions/pose)

---

**文档创建时间**: 2024年
**划分依据**: MediaPipe 三个独立的深度学习模型
**关键点数量**: 143 = 68 (face) + 21 (left_hand) + 21 (right_hand) + 33 (pose)



