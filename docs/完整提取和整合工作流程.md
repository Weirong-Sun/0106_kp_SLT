# PHOENIX 数据集完整提取和整合工作流程

## 完整工作流程

### 第一步：提取关键点（完整数据集）

```bash
# 使用 GPU 优化版本提取所有数据
# 注意：不使用 --max_samples_per_split 参数
python data/extract_phoenix_keypoints_gpu.py \
    --dataset_path /data/phd/wsun/datasets/PHOENIX-2014-T-release-v3/PHOENIX-2014-T \
    --output_path phoenix_keypoints_full.pkl \
    --num_gpus 4 \
    --num_workers_per_gpu 1 \
    > extraction_full.log 2>&1 &

# 监控进度
tail -f extraction_full.log

# 定期检查进度
python check_extraction_progress.py phoenix_keypoints_full.pkl
```

**估算时间**: 约 66-88 小时（2.7-3.7 天）

### 第二步：验证提取结果

```bash
# 查看提取结果统计
python view_keypoints.py phoenix_keypoints_full.pkl --detailed

# 检查数据集大小对比
python analyze_dataset_size.py --result_pkl phoenix_keypoints_full.pkl

# 可视化一些样本验证质量
python visualize_phoenix_keypoints.py phoenix_keypoints_full.pkl \
    --num_samples 10 \
    --splits train dev test
```

### 第三步：准备模型输入数据

```bash
# 转换为模型可直接使用的格式
python data/prepare_model_input.py \
    --keypoints_file phoenix_keypoints_full.pkl \
    --output_dir model_input_data

# 这会生成：
# - model_input_data/train_keypoints.npz
# - model_input_data/dev_keypoints.npz
# - model_input_data/test_keypoints.npz
# - model_input_data/*_stats.json
```

### 第四步：在模型中使用数据

```python
# 在训练脚本中加载数据
import numpy as np

# 加载训练集
train_data = np.load('model_input_data/train_keypoints.npz')
train_keypoints = train_data['keypoints']  # [N_train, 143, 3]
train_video_ids = train_data['video_ids']
train_image_paths = train_data['image_paths']

# 加载验证集
dev_data = np.load('model_input_data/dev_keypoints.npz')
dev_keypoints = dev_data['keypoints']  # [N_dev, 143, 3]

# 加载测试集
test_data = np.load('model_input_data/test_keypoints.npz')
test_keypoints = test_data['keypoints']  # [N_test, 143, 3]

print(f"训练集: {train_keypoints.shape}")
print(f"验证集: {dev_keypoints.shape}")
print(f"测试集: {test_keypoints.shape}")

# 数据已经准备好，可以直接用于模型训练
```

## 分批处理工作流程（可选）

如果不想一次性处理所有数据，可以分批处理：

### 方案 1: 按划分分批处理

```bash
# 1. 先处理训练集（最大）
python data/extract_phoenix_keypoints_gpu.py \
    --dataset_path /data/phd/wsun/datasets/PHOENIX-2014-T-release-v3/PHOENIX-2014-T \
    --output_path phoenix_keypoints_train.pkl \
    --splits train \
    --num_gpus 4 \
    > extraction_train.log 2>&1 &

# 2. 处理验证集和测试集
python data/extract_phoenix_keypoints_gpu.py \
    --dataset_path /data/phd/wsun/datasets/PHOENIX-2014-T-release-v3/PHOENIX-2014-T \
    --output_path phoenix_keypoints_dev_test.pkl \
    --splits dev test \
    --num_gpus 4 \
    > extraction_dev_test.log 2>&1 &

# 3. 合并结果
python data/merge_keypoint_results.py \
    --input_files phoenix_keypoints_train.pkl phoenix_keypoints_dev_test.pkl \
    --output_path phoenix_keypoints_full.pkl
```

### 方案 2: 按视频数量分批处理

如果数据集太大，可以先处理部分视频验证流程：

```bash
# 测试：只处理每个划分的前 100 个视频
python data/extract_phoenix_keypoints_gpu.py \
    --dataset_path /data/phd/wsun/datasets/PHOENIX-2014-T-release-v3/PHOENIX-2014-T \
    --output_path phoenix_keypoints_test.pkl \
    --max_samples_per_split 100 \
    --num_gpus 4

# 验证结果正确后，处理完整数据集
python data/extract_phoenix_keypoints_gpu.py \
    --dataset_path /data/phd/wsun/datasets/PHOENIX-2014-T-release-v3/PHOENIX-2014-T \
    --output_path phoenix_keypoints_full.pkl \
    --num_gpus 4
```

## 数据格式说明

### 原始提取格式 (phoenix_keypoints_full.pkl)

```python
{
    'train': {
        'keypoints': [{'face': ..., 'left_hand': ..., ...}, ...],
        'image_paths': [...],
        'video_ids': [...]
    },
    'dev': {...},
    'test': {...},
    'stats': {...},
    'keypoint_info': {...}
}
```

### 模型输入格式 (model_input_data/*.npz)

```python
# train_keypoints.npz
{
    'keypoints': np.array([N, 143, 3]),  # 所有关键点组合
    'image_paths': [...],
    'video_ids': [...]
}

# 关键点顺序:
# [0:68]   - 面部关键点
# [68:89]  - 左手关键点
# [89:110] - 右手关键点
# [110:143] - 姿态关键点
```

## 快速参考命令

### 完整提取（推荐）

```bash
# 一步到位：提取并准备模型输入
python data/extract_phoenix_keypoints_gpu.py \
    --dataset_path /data/phd/wsun/datasets/PHOENIX-2014-T-release-v3/PHOENIX-2014-T \
    --output_path phoenix_keypoints_full.pkl \
    --num_gpus 4

# 准备模型输入
python data/prepare_model_input.py \
    --keypoints_file phoenix_keypoints_full.pkl \
    --output_dir model_input_data
```

### 检查进度

```bash
# 检查提取进度
python check_extraction_progress.py phoenix_keypoints_full.pkl

# 分析数据集大小
python analyze_dataset_size.py --result_pkl phoenix_keypoints_full.pkl

# 查看结果详情
python view_keypoints.py phoenix_keypoints_full.pkl --detailed
```

### 验证数据

```bash
# 可视化验证
python visualize_phoenix_keypoints.py phoenix_keypoints_full.pkl \
    --num_samples 10 \
    --splits train

# 检查模型输入格式
python -c "import numpy as np; d=np.load('model_input_data/train_keypoints.npz'); print(d['keypoints'].shape)"
```

## 性能优化

### 推荐配置

```bash
# 4 GPU × 1 进程/GPU = 4 个进程（推荐）
--num_gpus 4 --num_workers_per_gpu 1

# 估算时间: 66-88 小时
```

### 激进配置

```bash
# 4 GPU × 2 进程/GPU = 8 个进程（激进）
--num_gpus 4 --num_workers_per_gpu 2

# 估算时间: 33-44 小时
# 注意: 需要足够的 CPU 核心和内存
```

## 数据使用示例

### 在 PyTorch DataLoader 中使用

```python
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader

class KeypointDataset(Dataset):
    def __init__(self, split='train', data_dir='model_input_data'):
        data = np.load(f'{data_dir}/{split}_keypoints.npz')
        self.keypoints = torch.FloatTensor(data['keypoints'])  # [N, 143, 3]
        self.video_ids = data['video_ids']
        self.image_paths = data['image_paths']

    def __len__(self):
        return len(self.keypoints)

    def __getitem__(self, idx):
        return {
            'keypoints': self.keypoints[idx],  # [143, 3]
            'video_id': self.video_ids[idx],
            'image_path': self.image_paths[idx]
        }

# 创建数据加载器
train_dataset = KeypointDataset('train')
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)

# 使用
for batch in train_loader:
    keypoints = batch['keypoints']  # [batch_size, 143, 3]
    # ... 模型训练 ...
```

## 相关文档

- [GPU分布式关键点提取指南.md](./GPU分布式关键点提取指南.md)
- [GPU加速说明.md](./GPU加速说明.md)
- [数据集大小差异分析.md](./数据集大小差异分析.md)
- [分布式关键点提取分析.md](./分布式关键点提取分析.md)

---

**文档创建时间**: 2024年
**适用场景**: PHOENIX 数据集完整提取和模型输入准备


